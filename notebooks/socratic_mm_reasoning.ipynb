{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Socratic Models for Multimodal Reasoning\n",
    "## Introduction\n",
    "In this notebook, we use the Socratic models approach applied to multimodal reasoning tasks, i.e, chain-of-thought (CoT) reasoning & visual question-answering (VQA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# global\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import set_seed\n",
    "import openai\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# local \n",
    "sys.path.insert(0, '../')\n",
    "import scripts.image_captioning as ic\n",
    "from scripts.utils import get_device\n",
    "\n",
    "# Local imports\n",
    "from scripts.coco_caption_base_hp_tune import ImageCaptionerBaseline\n",
    "# extensions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_places starting!\n",
      "load_places took 0.0s!\n",
      "load_objects starting!\n",
      "load_objects took 0.0s!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4442caa00f64c7985f2f46198c3336c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate the baseline image captioner class.\n",
    "image_captioner = ImageCaptionerBaseline(set_type='demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seed & device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # set seed for reproducibility\n",
    "# set the device to use\n",
    "device = get_device()\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_places starting!\n",
      "load_places took 0.0s!\n",
      "load_objects starting!\n",
      "load_objects took 0.1s!\n"
     ]
    }
   ],
   "source": [
    "# instantiate managers\n",
    "clip_manager = ic.ClipManager(device=device)\n",
    "image_manager = ic.ImageManager()\n",
    "vocab_manager = ic.VocabManager()\n",
    "prompt_generator = ic.LmPromptGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m lm_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgpt\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;66;03m# gpt or huggingface transformer model\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5, EleutherAI/gpt-neox-20b\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m lm_manager \u001B[38;5;241m=\u001B[39m \u001B[43mic\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLmManager\u001B[49m\u001B[43m(\u001B[49m\u001B[43mversion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlm_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_api\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m openai\u001B[38;5;241m.\u001B[39mapi_key \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mgetenv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOPENAI_API_KEY\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: __init__() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "# setup lm\n",
    "lm_model = 'gpt' # gpt or huggingface transformer model\n",
    "# OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5, EleutherAI/gpt-neox-20b\n",
    "lm_manager = ic.LmManager(version=lm_model, use_api=True, device=device)\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create image & text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute place & objects features\n",
    "place_feats = clip_manager.get_text_emb([f'Photo of a {p}.' for p in vocab_manager.place_list])\n",
    "obj_feats = clip_manager.get_text_emb([f'Photo of a {o}.' for o in vocab_manager.object_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-thought reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "We use the [ScienceQA](https://scienceqa.github.io/) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load scienceQA dataset\n",
    "scienceQA_dataset = load_dataset('derek-thomas/ScienceQA', split='validation')\n",
    "# filter out samples with no image\n",
    "scienceQA_dataset = [sample for sample in scienceQA_dataset if sample['image'] is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize dataset samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: delete this cell in final version\n",
    "# viz samples\n",
    "# for i, sample in enumerate(scienceQA_dataset[670:680]):\n",
    "#     print(f'sample {i}:')\n",
    "#     plt.figure(figsize=(4, 4))\n",
    "#     plt.imshow(sample['image'])\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "#     # sample['image'].show()\n",
    "#     print('question:', sample['question'])\n",
    "#     print('choices:', sample['choices'])\n",
    "#     print('hint:', sample['hint'])\n",
    "#     # print('lecture:', sample['lecture'])\n",
    "#     print('answer:', sample['answer'])\n",
    "#     print('solution:', sample['solution'])\n",
    "#     print('-'*50)\n",
    "\n",
    "# load sample idxs from json\n",
    "# with open('../data/scienceqa/sample_idxs.json', 'r') as f:\n",
    "#     sample_idxs_file = json.load(f)\n",
    "#     sample_idxs = sample_idxs_file['vqa']\n",
    "# for idx in sample_idxs[0:20]:\n",
    "#     sample = scienceQA_dataset[idx]\n",
    "#     print(f'sample {idx}:')\n",
    "#     plt.figure(figsize=(4, 4))\n",
    "#     plt.imshow(sample['image'])\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "#     # sample['image'].show()\n",
    "#     print('question:', sample['question'])\n",
    "#     print('choices:', sample['choices'])\n",
    "#     print('hint:', sample['hint'])\n",
    "#     # print('lecture:', sample['lecture'])\n",
    "#     print('answer:', sample['answer'])\n",
    "#     # print('solution:', sample['solution'])\n",
    "#     print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select & viz sample\n",
    "sample_idx = 142\n",
    "sample = scienceQA_dataset[sample_idx]\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(sample['image'])\n",
    "plt.show()\n",
    "print(f'question: {sample[\"question\"]}\\nchoices: {sample[\"choices\"]}\\nanswer: {sample[\"answer\"]}\\nsolution: {sample[\"solution\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot CoT reasoning\n",
    "Generate prompts using image info (CLIP) and questions, hints and choices from the dataset, along with a sentence to induce zero-shot CoT reasoning. Generate outputs (solution + rationale) from LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose prompt\n",
    "prompt = prompt_generator.create_cot_prompt(sample, clip_manager, vocab_manager, place_feats, obj_feats)\n",
    "print(f'prompt: {prompt}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate output\n",
    "if lm_model == 'gpt':\n",
    "    lm_params = {'max_tokens': 100, 'temperature': 1., 'n': 1}\n",
    "    output = ic.get_response_gpt(prompt, **lm_params)\n",
    "else:\n",
    "    lm_params = {'max_new_tokens': 50, 'temperature': 1., 'do_sample': True, 'length_penalty': 2.} \n",
    "    output = lm_manager.generate_response(prompt, lm_params)\n",
    "\n",
    "print(f'output: {output}\\ngt solution: {sample[\"solution\"]}\\ngt answer: {sample[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot CoT reasoning\n",
    "Generate prompts using image info (CLIP) and questions, hints and choices from the dataset, along with previous prompts & solutions (rational + choice) to induce few-shot CoT reasoning. Generate outputs from LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select & viz example & target samples\n",
    "# candidate pairs: single-prop: 122, 142 | multi-prop: 340, 142, \n",
    "# sample example\n",
    "eg_sample_idx, target_sample_idx = 142, 523\n",
    "eg_sample, target_sample = scienceQA_dataset[eg_sample_idx], scienceQA_dataset[target_sample_idx]\n",
    "# show example sample\n",
    "print('example sample:')\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(eg_sample['image'])\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f'question: {eg_sample[\"question\"]}\\nchoices: {eg_sample[\"choices\"]}\\nanswer: {eg_sample[\"answer\"]}\\nsolution: {eg_sample[\"solution\"]}')\n",
    "\n",
    "# show target sample\n",
    "print('target sample:')\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(target_sample['image'])\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f'question: {target_sample[\"question\"]}\\nchoices: {target_sample[\"choices\"]}\\nanswer: {target_sample[\"answer\"]}\\nsolution: {target_sample[\"solution\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate prompt\n",
    "prompt = prompt_generator.create_cot_prompt(eg_sample, clip_manager, vocab_manager, place_feats, obj_feats) + f'{eg_sample[\"solution\"]}. So the answer is {eg_sample[\"choices\"][eg_sample[\"answer\"]]}\\n' + prompt_generator.create_cot_prompt(target_sample, clip_manager, vocab_manager, place_feats, obj_feats)\n",
    "print(f'prompt: {prompt}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate outputs from LM\n",
    "if lm_model == 'gpt':\n",
    "    lm_params = {'max_tokens': 100, 'temperature': 1.}\n",
    "    output = ic.get_response_gpt(prompt, **lm_params)\n",
    "else:\n",
    "    lm_params = {'max_new_tokens': 100, 'temperature': 1., 'do_sample': False, 'length_penalty': 2.} \n",
    "    output = lm_manager.generate_response(prompt, lm_params)\n",
    "print(f'output: {output}\\ngt solution: {target_sample[\"solution\"]}\\ngt answer: {target_sample[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual question-answering (VQA)\n",
    "Visual question-answering using VLM (CLIP) + LM\n",
    "### Zero-shot VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample\n",
    "sample_idx = 148 # 134, 148\n",
    "sample = scienceQA_dataset[sample_idx]\n",
    "# show sample\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.imshow(sample['image'], aspect='auto')\n",
    "plt.show()\n",
    "print(f'question: {sample[\"question\"]}\\nchoices: {sample[\"choices\"]}\\nanswer: {sample[\"answer\"]}\\nsolution: {sample[\"solution\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose prompt\n",
    "prompt = prompt_generator.create_vqa_prompt(sample, clip_manager, vocab_manager, place_feats, obj_feats)\n",
    "print(f'prompt: {prompt}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate outputs from LM\n",
    "if lm_model == 'gpt':\n",
    "    lm_params = {'max_tokens': 20, 'temperature': 1.}\n",
    "    output = ic.get_response_gpt(prompt, **lm_params)\n",
    "else:\n",
    "    lm_params = {'max_new_tokens': 20, 'temperature': 1., 'do_sample': False} \n",
    "    output = lm_manager.generate_response(prompt, lm_params)\n",
    "print(f'output: {output}\\ngt answer: {sample[\"answer\"] }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select & viz example & target samples\n",
    "# sample example\n",
    "eg_sample_idx, target_sample_idx = 148, 134\n",
    "eg_sample, target_sample = scienceQA_dataset[eg_sample_idx], scienceQA_dataset[target_sample_idx]\n",
    "# show example sample\n",
    "print('example sample:')\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.imshow(eg_sample['image'], aspect='auto')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f'question: {eg_sample[\"question\"]}\\nchoices: {eg_sample[\"choices\"]}\\nanswer: {eg_sample[\"answer\"]}')\n",
    "\n",
    "# show target sample\n",
    "print('target sample:')\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.imshow(target_sample['image'], aspect='auto')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f'question: {target_sample[\"question\"]}\\nchoices: {target_sample[\"choices\"]}\\nanswer: {target_sample[\"answer\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate prompt\n",
    "prompt = prompt_generator.create_vqa_prompt(eg_sample, clip_manager, vocab_manager, place_feats, obj_feats) + f'{eg_sample[\"answer\"]}\\n' + prompt_generator.create_vqa_prompt(target_sample, clip_manager, vocab_manager, place_feats, obj_feats)\n",
    "print(f'prompt: {prompt}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate outputs from LM\n",
    "if lm_model == 'gpt':\n",
    "    lm_params = {'max_tokens': 100, 'temperature': 1.}\n",
    "    output = ic.get_response_gpt(prompt, **lm_params)\n",
    "else:\n",
    "    lm_params = {'max_new_tokens': 100, 'temperature': 1., 'do_sample': False, 'length_penalty': 2.} \n",
    "    output = lm_manager.generate_response(prompt, lm_params)\n",
    "print(f'output: {output}\\ngt solution: {target_sample[\"solution\"]}\\ngt answer: {target_sample[\"answer\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
