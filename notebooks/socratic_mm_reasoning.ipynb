{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Socratic Models for Image Captioning & Multimodal Reasoning\n",
    "## Introduction\n",
    "In this notebook, we use the Socratic models approach applied to image captioning and multimodal reasoning tasks, i.e, chain-of-thought (CoT) reasoning & visual question-answering (VQA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Local \n",
    "sys.path.insert(0, '../')\n",
    "import scripts.image_captioning as ic\n",
    "from scripts.utils import get_device\n",
    "# Extensions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seed & device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)    # set seed for reproducibility\n",
    "# set the device to use\n",
    "device = get_device()\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate managers\n",
    "clip_manager = ic.ClipManager(device=device)\n",
    "image_manager = ic.ImageManager()\n",
    "vocab_manager = ic.VocabManager()\n",
    "lm_manager = ic.LmManager(version='google/flan-ul2', use_api=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create image & text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute place & objects features\n",
    "place_feats = clip_manager.get_text_emb([f'Photo of a {p}.' for p in vocab_manager.place_list])\n",
    "obj_feats = clip_manager.get_text_emb([f'Photo of a {o}.' for o in vocab_manager.object_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image captioning\n",
    "We use extract info from an input image using CLIP and use to to construct a text summary, which is then fed as a prompt into the LM to generate captions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the image path\n",
    "img_dir = '../data/images/example_images'\n",
    "fname = 'astronaut_with_beer.jpg'\n",
    "img_path = f'{img_dir}/{fname}'\n",
    "\n",
    "# load image\n",
    "img = image_manager.load_image(img_path)\n",
    "# get image representation\n",
    "img_feats = clip_manager.get_img_emb(img)\n",
    "# show image\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get img info\n",
    "Extract image info using CLIP (zero-shot classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_type, n_people, location, sorted_obj_texts, obj_list, obj_scores = clip_manager.get_img_info(img, place_feats, obj_feats, vocab_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter unique objects\n",
    "Filter out unique objects using cosine similarity of their embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter unique objects\n",
    "filtered_objs = ic.filter_objs(sorted_obj_texts, obj_scores, clip_manager, obj_topk=10, sim_threshold=0.7)\n",
    "# filtered_objs = vlm.filter_objs_alt(vocab_manager.object_list, sorted_obj_texts, obj_feats, img_feats, clip_manager, obj_top=10)\n",
    "print(f'filtered objects: {filtered_objs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate captions\n",
    "Generate captions by composing a prompt using info extracted from CLIP, and use the LM to generate captions from the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n captions, order them and print out the best.\n",
    "n_captions = 20\n",
    "# Create a creative beautiful caption from this context:\n",
    "prompt = f'''This image is a {img_type}. It was taken in a {location}. It has {n_people}. It contains a {', '.join(filtered_objs)}. A caption I can generate to describe this image is:'''\n",
    "print(f'prompt: {prompt}')\n",
    "\n",
    "lm_params = {\"min_new_tokens\": 5, \"max_new_tokens\": 30, \"length_penalty\": 2, \"num_beams\": 8, \"no_repeat_ngram_size\": 3, \"temperature\": 0.9,  \"early_stopping\": True, \"do_sample\": True, \"num_return_sequences\": 1}\n",
    "\n",
    "caption_texts = lm_manager.generate_response([prompt] * n_captions, lm_params)\n",
    "\n",
    "# rank captions\n",
    "clip_manager.rank_gen_outputs(img, caption_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-thought reasoning\n",
    "### Data\n",
    "We use the [ScienceQA](https://scienceqa.github.io/) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load scienceQA dataset\n",
    "scienceQA_dataset = load_dataset('derek-thomas/ScienceQA', split='validation')\n",
    "# filter out samples with no image\n",
    "scienceQA_dataset = [sample for sample in scienceQA_dataset if sample['image'] is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show samples\n",
    "# good samples: 68, 90, 122\n",
    "for i, sample in enumerate(scienceQA_dataset[120:130]):\n",
    "    print(f'sample {i+1}:')\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(sample['image'])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    # sample['image'].show()\n",
    "    print('question:', sample['question'])\n",
    "    print('choices:', sample['choices'])\n",
    "    print('hint:', sample['hint'])\n",
    "    print('lecture:', sample['lecture'])\n",
    "    print('answer:', sample['answer'])\n",
    "    print('solution:', sample['solution'])\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample\n",
    "sample_idx = 122\n",
    "sample = scienceQA_dataset[sample_idx]\n",
    "# show sample\n",
    "# plt.figure(figsize=(5, 5))\n",
    "plt.imshow(sample['image'])\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "print('question:', sample['question'])\n",
    "print('choices:', sample['choices'])\n",
    "print('hint:', sample['hint'])\n",
    "print('lecture:', sample['lecture'])\n",
    "print('answer:', sample['answer'])\n",
    "print('solution:', sample['solution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get img info\n",
    "Extract image info using CLIP (zero-shot classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image info\n",
    "img_feats = clip_manager.get_img_emb(sample['image'])\n",
    "img_type, n_people, location, sorted_obj_texts, obj_list, obj_scores = clip_manager.get_img_info(sample['image'], place_feats, obj_feats, vocab_manager)\n",
    "# filter unique objects\n",
    "# filtered_objs = vlm.filter_objs(sorted_obj_texts, obj_scores, clip_manager, obj_topk=10, sim_threshold=0.7)\n",
    "filtered_objs = ic.filter_objs_alt(vocab_manager.object_list, sorted_obj_texts, obj_feats, img_feats, clip_manager)\n",
    "print(f'filtered objects: {filtered_objs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot CoT reasoning\n",
    "Generate prompts using image info (CLIP) and questions, hints and choices from the dataset, along with a sentence to induce zero-shot CoT reasoning. Generate outputs (solution + rationale) from LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n outputs from LM using prompt\n",
    "num_outputs = 5\n",
    "\n",
    "# compose prompt\n",
    "prompt = f'''This image is a {img_type}. It was taken in a {location}. It has {n_people}. It contains a {', '.join(filtered_objs)}.\n",
    "Question: {sample['question']}\n",
    "Choices: {sample['choices']}\n",
    "Hint: {sample['hint']}\n",
    "Answer: Let's think step by step...'''\n",
    "# Lecture: {sample['lecture']}\n",
    "print(f'prompt: {prompt}\\n')\n",
    "\n",
    "# generate outputs from LM\n",
    "lm_params = {\"min_new_tokens\": 5, \"max_new_tokens\": 30, \"length_penalty\": 2, \"num_beams\": 8, \"no_repeat_ngram_size\": 3, \"temperature\": 0.9,  \"early_stopping\": True, \"do_sample\": True, \"num_return_sequences\": 1}\n",
    "outputs = lm_manager.generate_response([prompt] * num_outputs, lm_params)\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f'{i + 1}. {output}')\n",
    "    \n",
    "print(f'\\ngt solution: {sample[\"solution\"]}\\ngt answer: {sample[\"answer\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
