{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Socratic Models for Multimodal Reasoning\n",
    "## Introduction\n",
    "In this notebook, we use the Socratic models approach applied to multimodal reasoning tasks, i.e, chain-of-thought (CoT) reasoning & visual question-answering (VQA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# global\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import set_seed\n",
    "import openai\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# local \n",
    "sys.path.insert(0, '../')\n",
    "import scripts.image_captioning as ic\n",
    "from scripts.utils import get_device\n",
    "\n",
    "# Local imports\n",
    "from scripts.image_captioning import ImageCaptionerBaseline, ScienceQaManager\n",
    "# extensions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_places starting!\n",
      "load_places took 0.0s!\n",
      "load_objects starting!\n",
      "load_objects took 0.1s!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b7d7b8e6144486a603ab9772ff1a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/ryanamaudruz/.cache/huggingface/datasets/derek-thomas___parquet/derek-thomas--ScienceQA-ca4903a3b5795914/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the baseline image captioner class.\n",
    "image_captioner = ImageCaptionerBaseline(set_type='mm_reasoning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-thought reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "We use the [ScienceQA](https://scienceqa.github.io/) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize dataset samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select & viz sample\n",
    "sample_idx = 142\n",
    "sample = image_captioner.science_qa_dataset[sample_idx]\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(sample['image'])\n",
    "plt.show()\n",
    "print(f'question: {sample[\"question\"]}\\nchoices: {sample[\"choices\"]}\\nanswer: {sample[\"answer\"]}\\nsolution: {sample[\"solution\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot CoT reasoning\n",
    "Generate prompts using image info (CLIP) and questions, hints and choices from the dataset, along with a sentence to induce zero-shot CoT reasoning. Generate outputs (solution + rationale) from LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose prompt\n",
    "# Generate the CLIP image embedding\n",
    "img_emb = image_captioner.clip_manager.get_img_emb(sample['image']).flatten()\n",
    "\n",
    "# Obtain a list of objects ordered by cosine similarity with the image\n",
    "sorted_obj_texts, obj_scores = image_captioner.clip_manager.get_nn_text(\n",
    "    image_captioner.vocab_manager.object_list, image_captioner.object_emb, img_emb\n",
    ")\n",
    "# Obtain a list of places ordered by cosine similarity with the image\n",
    "sorted_places, places_scores = image_captioner.clip_manager.get_nn_text(\n",
    "    image_captioner.vocab_manager.place_list, image_captioner.place_emb, img_emb\n",
    ")\n",
    "\n",
    "# Create the prompt\n",
    "prompt = self.prompt_generator.create_cot_prompt(sample, sorted_places, sorted_obj_texts)\n",
    "print(f'prompt: {prompt}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate output\n",
    "if lm_model == 'gpt':\n",
    "    lm_params = {'max_tokens': 100, 'temperature': 1., 'n': 1}\n",
    "    output = ic.get_response_gpt(prompt, **lm_params)\n",
    "else:\n",
    "    lm_params = {'max_new_tokens': 50, 'temperature': 1., 'do_sample': True, 'length_penalty': 2.} \n",
    "    output = lm_manager.generate_response(prompt, lm_params)\n",
    "\n",
    "print(f'output: {output}\\ngt solution: {sample[\"solution\"]}\\ngt answer: {sample[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot CoT reasoning\n",
    "Generate prompts using image info (CLIP) and questions, hints and choices from the dataset, along with previous prompts & solutions (rational + choice) to induce few-shot CoT reasoning. Generate outputs from LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select & viz example & target samples\n",
    "# candidate pairs: single-prop: 122, 142 | multi-prop: 340, 142, \n",
    "# sample example\n",
    "eg_sample_idx, target_sample_idx = 142, 523\n",
    "eg_sample, target_sample = scienceQA_dataset[eg_sample_idx], scienceQA_dataset[target_sample_idx]\n",
    "# show example sample\n",
    "print('example sample:')\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(eg_sample['image'])\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f'question: {eg_sample[\"question\"]}\\nchoices: {eg_sample[\"choices\"]}\\nanswer: {eg_sample[\"answer\"]}\\nsolution: {eg_sample[\"solution\"]}')\n",
    "\n",
    "# show target sample\n",
    "print('target sample:')\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(target_sample['image'])\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f'question: {target_sample[\"question\"]}\\nchoices: {target_sample[\"choices\"]}\\nanswer: {target_sample[\"answer\"]}\\nsolution: {target_sample[\"solution\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate prompt\n",
    "prompt = prompt_generator.create_cot_prompt(eg_sample, clip_manager, vocab_manager, place_feats, obj_feats) + f'{eg_sample[\"solution\"]}. So the answer is {eg_sample[\"choices\"][eg_sample[\"answer\"]]}\\n' + prompt_generator.create_cot_prompt(target_sample, clip_manager, vocab_manager, place_feats, obj_feats)\n",
    "print(f'prompt: {prompt}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate outputs from LM\n",
    "if lm_model == 'gpt':\n",
    "    lm_params = {'max_tokens': 100, 'temperature': 1.}\n",
    "    output = ic.get_response_gpt(prompt, **lm_params)\n",
    "else:\n",
    "    lm_params = {'max_new_tokens': 100, 'temperature': 1., 'do_sample': False, 'length_penalty': 2.} \n",
    "    output = lm_manager.generate_response(prompt, lm_params)\n",
    "print(f'output: {output}\\ngt solution: {target_sample[\"solution\"]}\\ngt answer: {target_sample[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual question-answering (VQA)\n",
    "Visual question-answering using VLM (CLIP) + LM\n",
    "### Zero-shot VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample\n",
    "sample_idx = 148 # 134, 148\n",
    "sample = scienceQA_dataset[sample_idx]\n",
    "# show sample\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.imshow(sample['image'], aspect='auto')\n",
    "plt.show()\n",
    "print(f'question: {sample[\"question\"]}\\nchoices: {sample[\"choices\"]}\\nanswer: {sample[\"answer\"]}\\nsolution: {sample[\"solution\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose prompt\n",
    "prompt = prompt_generator.create_vqa_prompt(sample, clip_manager, vocab_manager, place_feats, obj_feats)\n",
    "print(f'prompt: {prompt}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate outputs from LM\n",
    "if lm_model == 'gpt':\n",
    "    lm_params = {'max_tokens': 20, 'temperature': 1.}\n",
    "    output = ic.get_response_gpt(prompt, **lm_params)\n",
    "else:\n",
    "    lm_params = {'max_new_tokens': 20, 'temperature': 1., 'do_sample': False} \n",
    "    output = lm_manager.generate_response(prompt, lm_params)\n",
    "print(f'output: {output}\\ngt answer: {sample[\"answer\"] }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select & viz example & target samples\n",
    "# sample example\n",
    "eg_sample_idx, target_sample_idx = 148, 134\n",
    "eg_sample, target_sample = scienceQA_dataset[eg_sample_idx], scienceQA_dataset[target_sample_idx]\n",
    "# show example sample\n",
    "print('example sample:')\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.imshow(eg_sample['image'], aspect='auto')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f'question: {eg_sample[\"question\"]}\\nchoices: {eg_sample[\"choices\"]}\\nanswer: {eg_sample[\"answer\"]}')\n",
    "\n",
    "# show target sample\n",
    "print('target sample:')\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.imshow(target_sample['image'], aspect='auto')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f'question: {target_sample[\"question\"]}\\nchoices: {target_sample[\"choices\"]}\\nanswer: {target_sample[\"answer\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate prompt\n",
    "prompt = prompt_generator.create_vqa_prompt(eg_sample, clip_manager, vocab_manager, place_feats, obj_feats) + f'{eg_sample[\"answer\"]}\\n' + prompt_generator.create_vqa_prompt(target_sample, clip_manager, vocab_manager, place_feats, obj_feats)\n",
    "print(f'prompt: {prompt}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate outputs from LM\n",
    "if lm_model == 'gpt':\n",
    "    lm_params = {'max_tokens': 100, 'temperature': 1.}\n",
    "    output = ic.get_response_gpt(prompt, **lm_params)\n",
    "else:\n",
    "    lm_params = {'max_new_tokens': 100, 'temperature': 1., 'do_sample': False, 'length_penalty': 2.} \n",
    "    output = lm_manager.generate_response(prompt, lm_params)\n",
    "print(f'output: {output}\\ngt solution: {target_sample[\"solution\"]}\\ngt answer: {target_sample[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seed & device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)    # set seed for reproducibility\n",
    "# set the device to use\n",
    "device = get_device()\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate managers\n",
    "clip_manager = ic.ClipManager(device=device)\n",
    "image_manager = ic.ImageManager()\n",
    "vocab_manager = ic.VocabManager()\n",
    "prompt_generator = ic.LmPromptGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup lm\n",
    "lm_model = 'gpt' # gpt or huggingface transformer model\n",
    "# OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5, EleutherAI/gpt-neox-20b\n",
    "lm_manager = ic.LmManager(version=lm_model, use_api=True, device=device)\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create image & text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute place & objects features\n",
    "place_feats = clip_manager.get_text_emb([f'Photo of a {p}.' for p in vocab_manager.place_list])\n",
    "obj_feats = clip_manager.get_text_emb([f'Photo of a {o}.' for o in vocab_manager.object_list])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
