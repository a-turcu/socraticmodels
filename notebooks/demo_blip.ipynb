{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b9511a",
   "metadata": {},
   "source": [
    "# Demonstration of BLIP on Image Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74a311",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff717b4",
   "metadata": {},
   "source": [
    "This notebook is an exploration how the outputs one can expect using BLIP.\n",
    "\n",
    "It first shows the output if an image is passed without prompt. Then, it shows the output when both\n",
    "a prompt and an image are passed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c510a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a939ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Depending on the platform/IDE used, the home directory might be the socraticmodels or the\n",
    "# socraticmodels/scripts directory. The following ensures that the current directory is the scripts folder.\n",
    "try:\n",
    "    os.chdir('scripts')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "from scripts.image_captioning import ImageManager, BlipManager\n",
    "from scripts.utils import get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28928b4",
   "metadata": {},
   "source": [
    "## Instantiate the BLIP and image manager classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3278cc9c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f8f202599547fca13e87f5003e94cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d205600668401b91df1482f10ddf59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f945a42dbf94939b9c4210461deb35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee800b994af49c19eee94865b2d89c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba77615f87d40aba943b77ffd6de501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d84bf81dc9a44a4a5dcb69858c3054c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9cfd7f3f3949909306106286249993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the device to use\n",
    "device = get_device()\n",
    "\n",
    "# Instantiate the BLIP2 manager\n",
    "blip_manager = BlipManager(device)\n",
    "\n",
    "# Instantiate the image manager\n",
    "image_manager = ImageManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12921f6",
   "metadata": {},
   "source": [
    "### Load the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c301af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder = '../data/images/example_images/'\n",
    "img_file = 'astronaut_with_beer.jpg'\n",
    "img_path = img_folder + img_file\n",
    "image = image_manager.load_image(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5007d19",
   "metadata": {},
   "source": [
    "### Set the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fdebbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'max_length': 40,\n",
    "    'no_repeat_ngram_size': 2,\n",
    "    'repetition_penalty': 1.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22abc80",
   "metadata": {},
   "source": [
    "### Example 1: Caption without a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be512c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milenakapralova/miniconda3/envs/newdl2/lib/python3.9/site-packages/transformers/generation/utils.py:2305: UserWarning: MPS: no support for int64 for min_max, downcasting to a smaller data type (int32/float32). Native support for int64 has been added in macOS 13.3. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1678777875611/work/aten/src/ATen/native/mps/operations/ReduceOps.mm:1118.)\n",
      "  if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP2 caption without prompt: \"astronaut drinking beer on the moon\"\n"
     ]
    }
   ],
   "source": [
    "caption = blip_manager.generate_response(image, model_params=model_params)\n",
    "print(f'BLIP2 caption without prompt: \"{caption}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a0e8f",
   "metadata": {},
   "source": [
    "### Example 2: Asking questions to BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "785d009d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt input: \"Question: where is the picture taken? Answer:\" - BLIP Output: \"question : where is the picture taken? answer : astronaut\"\n",
      "Prompt input: \"Question: what are the different objects in the image? Answer:\" - BLIP Output: \"question : what are the different objects in the image? answer :\"\n",
      "Prompt input: \"Question: Who is in the image? Answer:\" - BLIP Output: \"question : who is in the image? answer : astronaut\"\n"
     ]
    }
   ],
   "source": [
    "question1 = \"Question: where is the picture taken? Answer:\"\n",
    "response1 = blip_manager.generate_response(image, prompt=question1, model_params=model_params)\n",
    "print(f'Prompt input: \"{question1}\" - BLIP Output: \"{response1}\"')\n",
    "\n",
    "question2 = \"Question: what are the different objects in the image? Answer:\"\n",
    "response2 = blip_manager.generate_response(image, prompt=question2, model_params=model_params)\n",
    "print(f'Prompt input: \"{question2}\" - BLIP Output: \"{response2}\"')\n",
    "\n",
    "question3 = \"Question: Who is in the image? Answer:\"\n",
    "response3 = blip_manager.generate_response(image, prompt=question3, model_params=model_params)\n",
    "print(f'Prompt input: \"{question3}\" - BLIP Output: \"{response3}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dac5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
